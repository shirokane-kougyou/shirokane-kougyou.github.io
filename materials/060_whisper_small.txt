'では今回のお題は最近話題の ステーブルディフュージョンに並んでというか続き手というかで話題のウィスパーさんのお話の技術解析を同じく 広川先生からお願いしたいと思いますはいよろしくお願いしますそうですねちょうどステーブルディフュージョン という流行ったその週とか次の週とかにまたオープンAIから今度は音声ですね音声処理ですごい精度 半花声もかなり高いって今ウィスパーでモデルがオープンAIから公開されたのでもうこちらもはい 論文読んで少しどんなアルゴリズムでどんな特徴があるのか今回ぜひお話できればと思いますよろしくお願いしますお願いしますでは早速概要なんですけれどもはいまあこちらオープンAIが発表したウィスパーっていうモデルというかアルゴリズムとかそういった成果ですね 特徴として68万時間分の音声データ他限が英語以外にも日本語とかドイツ語とかいろんな68万時間分のまあ大部分は多分英語なんですけど多様な音声データを学習することで人に引っ適する精度ってませいドプラスロバスト性いろんな環境ノイズが乗ってたり そういったいろんな環境のでもロバストな音声認識が可能になりましたっていうモデルですとでプラスですねこの方法って多様なデータセットの音形にフォーカスするためにモデルがアーキテクチャですねこちらはシンプルなトランソーマー遠行だデコーダーアーキテクチャっていうのを使っておりますでプラスメインだと割と英語で音声聞いてその英語の書き起こしっていうのがメインなんですけど学習時にはですねこの英語音声からテキスト英語テキスト書き起こしの他に入力音声の言語の特定だったり他の英語以外にも日本語音声聞いて日本語にテキスト書き起こしとかも含めてまあいろんなマルチタスクってかつマルチな言語の学習を行っております結果としてですね英語を含む99種類の言語に対して高い精度と安定性で音声の書き起こしや日本語のプラスですね日本語の音声とかを加えて英語の役をのテキストを出力するみたいのも可能になっているそうですいやーこれはマジで顧客が求めていたものの最終形態がついに来てくれましたねいやそうですねなんか僕もyoutubeとかって結構自動字幕みたいのあると思うんですけど割となんか適当というかそれまあこれが今の限界かなみたいに思う時あるんですけど結構このウィスパーウェブでブラウザで使えるデモとかも公開されてなんか僕も試しにポットキャスト収録してますみたいなことを話した時にほとんど完璧に文字起こししてくれてなるほどっていう感じでしたねいやーマジすごいなちょっと自分はポットキャストのなんでいやーこれじゃあポットキャストも全部文字起こしできて検索が可能になって類似とかも取れるようになってもうポットキャスト革命来たじゃんって思ってたけどでも確かにそれよりyoutubeの字幕つけるっていうその市場の方がでっかいですよね確かにいや僕はぜひやってほしいですねもうかなりyoutube見ているので確かに確かにじゃあ中身に行きますかどうやって彼らは文字起こしをしているのかそうですねはいなんですけどその前ですねまあ音声認識上でまあこれまでどういった研究があったかっていうところで論文で少し触れられていたのでもうこちらも少し話せればと思いますイントラクションですねはいこの音声認識まあ論文だとオートマティックスピーチレクグにしょまあそうですね英語だとそういうふうに言われるんですけどこういうするこれを行うアプローチではもちろんウィスパー以前も取り組まれていました例えば最近のですねgoogleの出した成果ビッグ sslっていうような成果だとこちらですねウィスパーが68万時間分なのに対してこのgoogleの最近の研究だと100万時間分のラベルなし音声データを使って教師なし学習してその上で目標の書物データにファインチューニングするっていうようなアプローチも提案されていましたとこっちのググの方がデータ量多いんですねじゃあそうですね僕もはいウィスパーで68万時間で多すげーってなしたんですけど一応なんかそうですねこの教師なしの方で100万時間使って学習したっていうのもあるっていうふうに書いてありますねでまただえっとまあファインチューニングによるアプローチで目標データの評価データに対してはまあスーパーヒーマン人間語エレベルの制度が出ましたよっていうような報告があるんですけどまあ一方でじゃあそれ以外のデータセットに適用するとちょっとあんまりまあスーパーヒーマンって言ってたわりには他のデータに適用するとうまくいかないっていうようなことも報告されているっていうのを言われておりますとなるほどなんでまあこのウィスパーの時点で目標としてはできるだけその応用先のデータセットに依存しないでファインチューニング不要で半括するモデルを獲得したいよねっていうふうに言われていますとでこちらですね今の100万時間っていうのは教師なしの音声だけ音オーディオだけで学習して目標データにファインチューニングするってアプローチだったんですけどまあ一方ですね教室機の音声データセットっていうのはまたいさすがに量が限られていますとでマロン分で話されていたのはきれいな教室機データセットだと5000時間くらいでもうちょっと緩いのいじな弱教室機データセットでも1万時間とか3万時間くらいのものしかないんですよねってふうに言われていましたとまあウィスパーではこういった教室機データセットと膨大なさらに膨大なデータセットのギャップを埋めるって目的で弱教室機音声データの量をどんもっとスケーラップして結果的に68万時間の多言語のデータセットを作って教師あり学習を行いましたとでこれによって最終的には特定のデータセットやのファインチューニングってのを行わずに今数色ん可能となっていて結果的に優れた半花性能っていうのを獲得しましたよっていうふうなのがざっくり語られていましたえあそうだったんだちなみに音声でいう教師付っていうのは本当にその音声データと実際に何に喋られたかっていうテキストがついてるっていうことなんですよねそうだとはい思いますねまたなんかさらに細かいとこの音声の何秒から何秒の間にこのテキストでみたいなのも気持ちいてるともっと詳細で学習しやすかったりするのかなと思いますねそうかでなんかてっきり680万時間っていうから教師なしなんだろうなって勝手に思ってたんですけど教師あり教師ありというか弱教師付音声データって何なんですかね一応あれですね68万時間ですね弱教師付っていうのはもうもちろん説明するんですけど基本的には音声に対してテキストがひもついているっていう状態ですねそう思ったけばいいんだそうですねはい基本的にそのペアでウィスパーも学習していくっていう状態ですねすごいなデータセット集めてきたんだ作ったのか集めたのかわかんないけどそうですね早速その辺のウィスパーの中身ついてちょっと説明していければと思いますウィスパーの中身ってことでデータセットとモデルのアーキテクチャと学習方法って3つ大まかに説明していければと思いますまずデータセットですね今回他の画像とかでもよくあると思うんですけど膨大なデータセットっていうのを確保するためにインターネットからテキストの書き起こしが付与された音声っていうのを集中していますとこの音声っていうのは色んな環境だったり録音設定だったり和射多分大人だったり若い人だったり男性女性っていうことかなって思いますねあとは英語以外にもいろんな言語をカバーするように集中していますとで単純に何も考えずに集中するってだけではなくてプラスですね一部の書き起こしインターネットから集中できる書き起こしっていうのは既存のテキスト書き起こしサービスが付与したものと想定されますとなのでそれに対して人手のそういった書き起こしと機械が自動的に付けた書き起こしラベル教師ですねが混在すると学習にちょっと悪影響があるっていうふうに言われているのでいくつかのヒューリースティックスビックリマークとか果てなマークだったり缶間のウムだったりすべておもしすべて小文字みたいなそういう惑星ですかねによってちょっとこれは機械によるテキスト付けっぽいっていうものをフィルタリングしてある程度好品質がデータセットっていうのを集めているとのことでしたなるほど機械が作ってたらもう覗いちゃってるんだそうですねでこのデータセットですねまあ大部分は英語音声英語テキストなんですけどそれに加えて他言語音声とその言語のテキストのペアも収集していますとそれに加えて他の言語も例えば日本語でそれに対して英語のテキストが付与されているっていうペアに加え日本語に他の言語プラス英語のテキストっていうペアに関しては英語の書き起こし学習用のデータとしてまた確保していてこれによって多言語かつマルチタスクの学習用のデータセットをそうして作成したっていう風になっているとのことです学習用にはもともと音声結構時間いろいろバラ付きあると思うので学習用には最終的に30秒ごとの音声のクリップに分割して学習に使ったとのことですこれって実際日本語も結構精度いいっていう話が出てくると思うんですけど日本語の教師ありのデータセットってどっから持ってきたですかねなんか具体的にはアプリンディクスとか見ればあるかもしれないですけどちょっとパッと見は具体的に日本語はここからとかはなかったですねなんかこれ論文に書いてあったわけじゃないんですけど結構このロンビスパーの論文読んでる時に歌のサイトとかって歌ってる内容と歌詞が紐付けられたりするので愛のもしかしたら使えるのかなとちょっと思ったりしましたねなるほどそれ面白いですねデータセットはこういった内容ですねそれに踏まえてモデルのアーキテクチャと学習方法に移りますこのウィスパーではですね先ほど用意した豊富なデータセットとマルチ言語マルチタスクの学習っていうのにフォーカスするためにこのモデルのアーキテクチャに関しては音声からテキストの書き起こしっていうセイクエンス2セイクエンスのスイロンが実行できてデータ量に対してスケールするアーキテクチャとしてシンプルな円コーダデコーダベースのトランスフォーマーを採用しているっていうふうになっておりますここがわりとそこまで複雑ではないですね円コーダの方ではですねもともと音声波形ですのでこれをログメルスペクトログラム時間かける周波数の表現ですねに変換した上で1次元のコンボリューション1Dとゲルーっていう活性化関数を通した上でこのトランスフォーマーでも一般に使われるポジショナルエンコーディングを通してトランスフォーマーエンコーダに入力しますこの辺シークエンス2シークエンスっていうところでザクオーマカにはテキストの翻訳のようなゲースとザクリア似ているかなと思いますねその上でデコーダですねこちらはテキストの書き起こしを順次予測するんですけれども順次予測しますとなんで音声入力してそれに対応するディス・イズ・アーみたいなテキストを予測していきますシンプルにはその上なんですけどプラス実際にはですねこの話されているオーディオの言語が何なのか英語だったのか日本語だったのかだったり初割を行きのタイムスタンプの予測などもテキストとまとめてトークン系列として扱って全部まるっとまとめて予測対象とするってことを行っているようですなるほどこれなんか実際にツイッターとかで試した人たちが上げてたんですけどそのこれは日本語だよって教えずにやってたのに普通に日本語として出力ちゃんとしてくれてるみたいなやつはそもそもモデルが学習している中にこの今学習で見せられているものが日本語であるっていうことも普通に吹きっくるめて学習しているってことなんですねそうですねはいその通りですねすごいなこれととにかくあれか学習の時にそのインプットは音のデータですけど実際にモデルが学習しているのはメルスペクトログラムだからあのよく音声系の人たちが画像出してるあの波打ったモニョモニョモニョみたいな画像を見せてこの画像のこういう絵だったらこの発音をしてるよっていうテキストを予想するっていう画像からテキストを予測するってことをやってるんですねまざっくり言えばはいそうなりますねはいそしてなんでこの辺の全部まとめてその言語の特定とかタイムスタンプとかも全部トークンとして系列と系列として統一系列として成形してトランスソーマーのセイクエンスとセイクエンスで学習しちゃうっていうのがうまいところでありトランスソーマーの結構柔軟で優秀のところなのかなっていうのは読んでて思ったところですねそうですよね全部ひっくるめても学習してってやってるってことですもんねなのでモデルのアーキテクチャーと学習方法もざっくりほとんどこれで以上って形ですねでまぁもともとのデータ68万時間って多様なので学習値のデータ拡張とかもまろん分ではしていないっていうふうに書いてありましたあとはもうほとんど実験結果とディスカッションですね基本的にはかなり精度がいいっていうがまあ大まかの内容なんですけどやり方としてはですね学習で68万時間学習していますともともとの目標としてファインチューニングなどによらない半花生のミルっていうのが目標だったので既存の各評価用データセットデータセットの評価セットを直接推論してそれで評価を行うっていう扱いをしています評価指標はですね一般的な音声認識の指標であるワードエラーレートを予測した単語系列が正解とちゃんとマッチしているかっていうような指標を使っています全部の実験結果はちょっと活哀するんですけどまあわかりやすいところとまず英語の音声認識ですね英語の音声認識だとリブリスピーチっていうデータセットが一般的なようですとでこのリブリスピーチだけを見るとですね必ずしもミスパーっていうのは既存のこのリブリスピーチで教師ありで学習したモデルの制度には及んでいませんとですけどもまあ一方でじゃあそれを踏まえてほかのデータセットも含めた平均的な半花生のを見てみましょうとデータセットいくつかあるわけですけどミスパーで全部評価してみるっていうのと既存の教師ありモデルでリブリスピーチで学習したモデルを他の色んなデータセットでも評価してみましょうとそれによって各データセットまたがあった平均的な半花生のを見てみましょうっていう扱いだと非常に優れた認識性のでなっていて人の認識制度この論文の中で書かれていたのは著者の一人のアレクラードフォードさんって今かなり有名な方の認識制度も指摘しているっていうふうに書かれていますとこれは著者の人が同じデータセットを聞いてちゃんと聞きなんすか認識できたかって言う制度も取ってるってことなんですよね多分そうですねはい面白いなるほどなんでそうですねやっぱり既存のモデル的教師ありとかファインチューニングベースのアプローチだとそのチューニングしたデータセットだとスイーパーヒューマンっていうようなレベルって言われていても他のデータセットも含めたなんか性能低点だと今一にやっぱりなるよねっていうふうに言われていますじゃあその英語の次に多言語の音声認識ですねで今それでもやっぱりする人式性能が確認されましたとで面白いのがですねこの学習データ68万時間あってその中にで各言語のデータ量ばらつきがあってやっぱり英語が一番多くてマイナナ言語だと結構少ないものがあるんですけどその相関学習データの量と各言語の学習データの量とその言語ごとのエラーの相関っていうのを見てみるとすごいかなりきれいに相関が取れますとまあ両対数グラフですがかなりまっすぐな相関があってやっぱりデータを増やすほどちゃんとエラーレートの制度もスケールしていくよねっていうのがわかってわかったっていうふうに報告されていますもうデータ量が正義みたいな世界なんですねそうですねこんななんかきれいに相関でるんだっていうふうに思いましたね確かにただしていえばそうですねえっとインドとかヨーロピアン系から離れた言語あとちょっと制度データを増やしても若干制度が明かりづらくてこの辺はまあ改良の余地ありだよねみたいなふうには言われていますねこれって実際にその68万時間のうちの英語は何時間分でとかって内訳も出ているんですかたまあったと思いますね半分以上は確か英語であそんなにプロットを見ると日本語が1万時間くらいって少ないのだと10時間くらいしか10時間1時間しかないようなのもあるっていうふうになっていますね日本語は1万時間なんだなんかすごい制度いいっていうけど68万時間のうちの1時間分しかデータはなかったんですねそうですねそんなもんなんだそんなもんって言っても全然多いんだろうけど1万時間分の日本語の音声のしかも教師ありってなると他にもいろいろ実験してるんですけどまさらっと説明していくと他に入力音声にノイズが乗ってしまっても既存モデルよりロバストですっていうのが結果もありますとあと既存の研究成果以外に照揚の音声書き起こしサービスとかと比べても損職ない制度ですよねっていうのも報告されていますとそれとは別にですねプロ人間のプロの書き起こしとも比べてみた時にそれにもかなり歴歴してますねっていうような結果も報告されていましたここまで実験結果ですねあのノイズにも強いっていうのは確かにデベロッパーズIOでブログが書かれていてそれなんかわざわざオンライン会議の音声のものに先風機の音を乗せて合成してそれでベースモデルとミディアムとラージのモデルでノイズにどれぐらい堅労なのかみたいな実験とかされてましたね確か結果はベースだと先風機の音入れたらちょっと下がっちゃうけどあとだったからミディアムとかラージはなんかもうほとんど影響がなかったっていうぐらいサウトノイズ状況してるみたいな感じで面白い結果でしたねはいはいはいまあやっぱなんかエンジニアだとちょっとそういう公開されてるものに意地悪したくなるのはちょっとわかるなって思いますねはいまあなので結構いろんな実験結果でかなり制度を高くかつかなりロバストっていうのが報告されていました一方でリミュータイションとかフィーチャーワークっていうのもいくつか語られていますと今回先ほどデータセットの準備っていうところで少し触れたんですけど学習でた基本30秒ごとで学習していますとウィスパーじゃそれ長い音声を処理するときにはどうしているかっていうとあれだとこれは内部に30秒ごとにスライドしながら処理するってことをやっていますとまあそのときにアピームサーチとかヒューリスティックス、工夫をいくつか持ちいてるんですけどここはまだまだ自然につないで長い音声を書き起こしできるように改善の意地がありますよねって言われていますともう一個ですねまだ完璧じゃないんだなって思った点として特に長い音声だと人間ならやらないような間違い書き起こしのテキストがループしたり最初とか最後の内容を無視していたり明らかに存在しないような単語がテキストに現れるってことを報告されていてこの辺はより優れたデコーディングが必要ですよねって報告されていましたあとは細かいところだとモデルアーキテクチャー今回かなりシンプルな円コーダ・デコーダ・トランス・フォーマーですっていうところだったりデータ拡張もしてないですってところだったりあるのでその辺の改善だったりあとは既存の自己教師あり学習とかと組み合わせっていうのも応用先としてはありですよねってふうに書かれていましたなるほどこれってそういえばOSSだからコード取ってきて回せますけどCPUとかでもベースモデルだったら回るみたいな結構軽いんですよね確かなんかそんな話だった気がするそうですねなんかそれぞれメモリの大きさ書いてあって一番大きいのでも中ギガとかだった気がするので回る分にははいもちろん処理は遅くなっちゃうかなと思いますけど回る分には回るのかなと思いますねGPU持ってきたらもっと早く回るってことですよねそしてそうですねはいもちろんそのはずだと思いますなるほどななんかツイッターであと見たのはm1Macであのリビルド宮川さんm1Macで3時間ぐらいの音声を文字起こししようとしたら結構なんかまる1日かかったみたいなツイートされた気がしますねそれがでもちょっと大きいモデルだったのかなラージじゃなくてミリアムかベースみたいな話だったと思いますけど長い音源になったらそのCPUとかだったま全然時間かかるんだなっていうポイですねなるほどなるほどそうですねはいありがとうございます以上を含めてまとめに移りますと最初にお話ししておりまウィスパー豊富な多言語のデータセットでまシンプルな円行なデコーダベースのトランソンマモデルでマルチタスクの学習を行いましたと結果的に人間に知ってきする精度とロバスト性の音声入識の力を獲得できましたっていう風に報告されていますと途中途中もちょっと言ったんですけど私の感想としてはですね豊富なデータセット用意しつつそれだけじゃなくてちゃんとできるだけ定品質のデータっていうのは除外していたりするところなったり英語の書き起こし以外にも多言語の書き起こしだったり言語の特定などいろんなタスクっていうのシンプルなセイクエンス2セイクエンスの枠組みに落とし込んでスケーラブルなモデルを獲得しているっていう点がかなり高権として大きいのかなっていう風に感想として思いましたってところですねこれって特に何がすごかったかっていう意味で言うとゴミっぽいデータはちゃんと排除した上でかなり膨大な教師ありの音声データを使ったからっていう貢献が結構サブンとしてでっかいんですかね従来研究とはそうですねやっぱりそれ大きいと思いますねあとはそれでそのマルチタスクの学習をうまくシンプルな形に落とし込んでシンプルなエンコードで行こうだトランスソーマーで学習しているっていうのが結構大きいんじゃないかなと思いますなんかモデルはそんなに大したことない風に聞こえたんですけどいやモデルもちゃんとマルチタスクのっていうところですごい奴なんだよっていうことなんですかねそうですねまあ多分このモデルのアーキテクチャーだけで言えば割と結構簡単に実装できるんじゃないかなと思いますねそのマルチタスクのデータをうまく加工整形するところが割と難しそうかなってふうに思いましたねあとあのなんか初めの方の話なんですけど気になったのはGoogleの100万時間の方で描き出したビッグSSLでしたっけあれは精度もなんか言うほどよくなかったって話だったんですけどそっちのビッグSSLの方はそしたらデータ量はちゃんとあったのにモデルがなんかあんまりいけてなかったってことになるんですかねもっといろいろ違いはあると思うんですけどそうですねえーまあ多分やっぱり教師なしだとちょっと限界があるのかなっていうのと結局そのファインチューニング先にオーバーフィットしちゃうっていうのがウィスパーとのサブンとしてはかなり大きいんじゃないかなってふうに思いますねまあ多分精度もそんなに悪いってほどじゃないと思うんですけど反応な色んなシーンのデータに今は繁華して水論できるっていう意味だと教師ありで大きなデータを用意して学習したウィスパーに群配が上がっているのかなと思いますねそんな感じなんですねベースとミディアムとラージって言って結構使いやすい形でローカルマシンでも回せる形提供してくれてるからまたそこもなんかこうだけ話題に差が出たっていうのは貢献してるんですかねステーブルディフュージョンと一緒で姿勢の人たちも回しやすいっていう意味で結局そうですそのピックSSL公開されたとしても教師なしの状態で公開されたとしても自分で使うにはファインチューニングしなきゃいけないですしファインチューニングされたものだとファインチューニング先のデータにオーバーフィットするっていう形になっているので完全にすぐクローンして使えるウィスパーがかなり注目されているのかなと思いますねなるほどいやありがとうございますちょっとこれこの収録までに白金FM試しにやってみあの文字起こししてみてすごいっていう話をここに添えられたらいいと思ったがマニアーズなんでちょっと追加の宿題でやっとこうと思いますそうなんですよねちなみに今まで白金FMは投稿するときにあの文字起こししたワードクラウドをツイートにくっつけてたんですけどあれはずっとずっとというか一応公開されている文字起こしのAzureとAWSとGCPの文字起こしのAPIを使って比較して一番AWSのやつが使いやすかったのでで文字起こしの品質も結構どんぐりの性比べ的な感じではあるんですけどマシだったからそれ使ってたんですけどもう今回からはウィスパーで全部回したいなと思いますねそんなに違うんだったらということでめっちゃ楽しみそうですねちょっとそれはぜひ見てみたいですねうんそうですよねあとなんか今までのクラウドサービスねつってやっぱりクラウドにデータを置くのでその社内の秘密の会話とかはやっぱり簡単に上げて文字起こししようみたいなのできなかったですけど今回これもローカルで回せるから社内用のなんかおもちゃツール作ってそこに投稿したらみんな議事録文字起こしできるようみたいなまあラージュモデルで結構重いけどみたいなもん含めてできるからこれなってんすかなんかこれみんなが求めてたやつですごい業務効率みたいなのにももっと使えると思ってもっと話題になると思ったんですけどなんか微妙にちょっとしたびになってきますまた一盛り上がりした後にそうですねこれの理活用事例みたいなのめっちゃ楽しみにしてますねタイムラインにいっぱい出てくるのかなりそうですよね多様なやちは広そうな気はしますねあとなんかこれちょっと関係ないんですけど結構似たようなタイミングでフェイスブックの研究開発部署がR&Dのところが作ったなんて読むんだろうDemkas V3かなちょっと読み方間違えてる気がしますけどそれを音源分離ができるっていうやつが出ててかなりきれいにバンドの音とボーカルの音を切り分けたりとかできてたんですけどなんかそんなこともかなりきれいにできて文字起こしもちゃんとできるんだったら音声系のデータ基本的にもう制覇したと言っても過言ではないのではって思ったりするんですけど会話の人たち的にはどうなんですかねいやどうなんですかね僕も正直音声はほとんど触れてなかったので分からないところではあるんですけどあれもToye結構そうして例えばWhisperだと明らかに話されてないタウンが出てくるとかなんかループしてしまうとかかなりなんか寝深い問題そうなのでこの辺まだやっぱり改善の位置とかまだまだあるのかなと思いますねそうですね デコーダー側が改善されてもうちょっと長く学習と水論を行うような感じになってきたらまたという間に良いやつが出るんでしょうねそうですね はい かなり期待できると思いますいやありがとうございました ちょっとぜひ早めに試してこんなにもAWSとかのAPIと違うっていうのを比較したいですね自分もそうですね はい 楽しみですねじゃあ結構サクッと終わっちゃいましたがこんなとこですかねではでは今回はこれで終わりたいと思いますありがとうございました'
