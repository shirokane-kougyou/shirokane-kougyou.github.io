'では今回のお題は、最近話題のStable Diffusionに並んでというか続きてというかで、話題のWhisperさんのお話の技術解析を同じく、広川先生からお願いしたいと思います。はい、よろしくお願いします。そうですね、ちょうどStable Diffusionが流行ったその週とか次の週とかに、またOpenAIから、今度は音声ですね。音声処理で、すごい精度で、反響性もかなり高いっていう、WhisperっていうモデルがOpenAIから公開されたので、こちらも論文読んで、少しどんなアルゴリズムでどんな特徴があるのかを、今回是非お話しできればと思います。よろしくお願いします。お願いします。では早速概要なんですけれども、こちらOpenAIが発表したWhisperっていうモデルというか、アルゴリズムというか、そういった成果ですね。特徴として、こちら68万時間分の音声データ、多言語、英語以外にも、日本語とかドイツ語とかいろんな68万時間分の、大部分は多分英語なんですけど、多様な音声データを学習することで、人に匹敵する精度、精度プラスロバスト性、いろんな環境、ノイズが乗ってたり、そういったいろんな環境でもロバストな音声認識が可能になりましたっていうモデルです。プラスですね、この豊富で多様なデータセットの恩恵にフォーカスするために、モデル、アーキテクチャですね、こちらはシンプルなトランスフォーマー、エンコーダー、デコーダー、アーキテクチャーっていうのを使っております。プラス、メインだと割と英語で音声聞いて、その英語の書き起こしっていうのがメインなんですけど、学習時にはですね、この英語音声から英語テキスト書き起こしの他に、入力音声の言語の特定だったり、他の英語以外にも日本語音声聞いて、日本語テキスト書き起こしとかも含めて、いろんなマルチタスク、てかつマルチな言語の学習を行っております。結果としてですね、英語を含む99種類の言語に対して、高いせいだと安定性で音声の書き起こしや、日本語の音声とかを加えて、英語の訳のテキストを出力するみたいなのも可能になっているそうです。これはマジで顧客が求めていたものの最終形態がついに来てくれましたね。いやそうですね、僕もYouTubeとかって結構自動字幕みたいのあると思うんですけど、割となんか適当というか、それ、まあこれが今の限界かなみたいに思う時あるんですけど、結構このWISPAR、ウェブでブラウザで使えるデモとかも公開されてて、なんか僕も試しにポッドキャスト収録してますみたいなことを話したときに、ほとんど完璧に文字起こししてくれて、なるほどっていう感じでしたね。いやーマジすごいな、ちょっと自分はポッドキャスト能なんで、いやーこれじゃあポッドキャストも全部文字起こしできて、検索が可能になって、類似とかも取れるようになって、ポッドキャスト革命きたじゃんって思ってたけど、でも確かにそれよりYouTubeの字幕付けるっていうその市場の方がでっかいですよね、確かに。僕は是非やってほしいですね。かなりYouTube見ているので。確かに確かに。じゃあ中身に行きますか。どうやって彼らは文字起こしをしているのか。そうですね、はい。なんですけどその前にですね、まあ音声認識で、これまでどういった研究があったかっていうところで、論文で少し触れられていたので、こちらも少し話せればと思います。イントロダクションですね。この音声認識、論文だとAutomatic Speech Recognition、英語だとそういう風に言われるんですけど、これを行うアプローチというのは、もちろんウィスパー以前も取り組まれていましたと。例えば最近のですね、Googleの出した成果、BigSSLっていうような成果だと、こちらですね、ウィスパーが68万時間分なのに対して、このGoogleの最近の研究だと、100万時間分のラベルなし音声データを使って、教師なし学習して、その上で目標の初期のデータにファインチューニングするっていうようなアプローチも提案されていました。こっちのGoogleの方がデータ量多いんですね。そうですね、僕もウィスパーで68万時間で、おーすげーって言われてたんですけど、一応教師なしの方で100万時間使って学習したっていうのもあるっていう風に書いてありますね。ただファインチューニングによるアプローチで目標データの評価データに対しては、スーパーヒューマン、人間語彙レベルの制度が出ましたよっていうような報告があるんですけど、一方でそれ以外のデータセットに適用すると、ちょっとあんまりスーパーヒューマンって言ってた割には、他のデータに適用するとうまくいかないっていうようなことも報告されているっていうのは言われておりますと。なるほど。なので、このウィスパーの時点で目標としては、できるだけその応用先のデータセットに依存しないで、ファインチューニング不要で繁華するモデルを獲得したいよねっていう風に言われていますと。こちらですね、今の100万時間っていうのは教師なしの音声だけ、オーディオだけで学習して目標データにファインチューニングするってアプローチだったんですけど、一方で教室付きの音声データセットっていうのはさすがに量が限られていますと。論文で話されていたのは、きれいな教室付きデータセットだと5000時間くらい。もうちょっとゆるい、ノイジーな弱教室付きデータセットでも、1万時間とか3万時間くらいのものしかないんですよねっていう風に言われていましたと。ウィスパーではこういった教室付きデータセットと、さらに膨大なデータセットのギャップを埋めるという目的で、弱教室付き音声データの量をもっとスケールアップして、結果的に68万時間の多言語のデータセットを作って教師あり学習を行いましたと。これによって最終的には特定のデータセットへのファインチューニングを行わずに推論可能となっていて、結果的に優れた反過性能を獲得しましたよという風なのがざっくり語られていました。そうだったんだ。ちなみに音声でいう教室付きっていうのは、本当にその音声データと実際に何にしゃべられたかっていうテキストがついてるっていうことなんですよね。そうなのかと思いますね。あとはさらに細かいと、この音声の何秒から何秒の間にこのテキストでみたいなものが気が付いてると、もっと詳細で学習しやすかったりするのかなと思いますね。そうか。てっきり680万時間って言うから教師なしなんだろうなって勝手に思ってたんですけど、弱教師付き音声データって何なんですかね。まあ一応あれですね、680,000時間ですね。弱教師付きっていうのは、もちろん説明するんですけど、基本的には音声に対してテキストが紐づいているっていう状態ですね。そう思っておけばいいんだ。なるほど。基本的にそのペアでウィスパーも学習していくっていう状態ですね。すごいな。データセット集めてきたんだ。作ったのか集めたのかわかんないけど。そうですね。早速その辺のウィスパーの中身について説明していければと思います。ウィスパーの中身ってことで、データセットとモデルのアーキテクチャと学習方法っていう3つを大まかに説明していければと思います。まずデータセットですね。今回他の画像とかでもよくあると思うんですけど、膨大なデータセットっていうのを確保するために、インターネットからテキストの書き起こしが付与された音声っていうのを収集しています。この音声っていうのは、いろんな環境だったり、録音設定だったり、和射、多分大人だったり、若い人だったり、男性、女性っていうことかなって思いますね。あとは英語以外にもいろんな言語をカバーするように収集しています。単純に何も考えずに収集するってだけではなくて、プラス一部の書き起こし、インターネットから収集できる書き起こしっていうのは、既存のテキスト書き起こしサービスが付与したものと想定されます。なのでそれに対して人でのそういった書き起こしと、機械が自動的に付けた書き起こし、ラベル、教師ですね、が混在すると学習にちょっと悪影響があるっていう風に言われているので、いくつかのヒューリスティクス、ビックリマークとかハテナマークだったり、カンマのウムだったり、全て重し全て小文字みたいなそういう癖ですかね。によって、これは機械によるテキスト付けっぽいっていうものをフィルタリングして、ある程度高品質なデータセットっていうのを集めているとのことでした。なるほど、機械が付くってやつはもう覗いちゃってるんだ。そうですね。このデータセットですね、メイン大部分は英語音声、英語テキストなんですけど、それに加えて他言語音声とその言語のテキストのペアも収集しています。それに加えて他の言語、例えば日本語で、それに対して英語のテキストが付与されているっていうペアに関して、他の言語プラス英語のテキストっていうペアに関しては英訳、英語の書き起こし学習用のデータとして確保していて、これによって多言語かつマルチタスクの学習用のデータセットを総じて作成したっていうふうになっているとのことです。学習用には、もともと音声、結構時間、いろいろバランスがあると思うので、学習用には最終的に30秒ごとの音声のクリップに分割して学習に使ったとのことです。これって実際日本語も結構精度いいっていう話が出てくると思うんですけど、日本語の教師ありのデータセットってどこから持ってきたですかね?具体的にはアペンディクスとか見ればあるかもしれないですけど、ちょっとパッと見は、具体的に日本語はここからとかはなかったですね。これ論文に書いてあったわけじゃないんですけど、結構このロンピスパーの論文読んでるときに、歌のサイトとかって歌ってる内容と歌詞が紐付けられてたりするんで、ああいうのもしかしたら使えるのかなとちょっと思ったりしましたね。なるほど、それ面白いですね。データセットはこういった内容ですね。それに踏まえてモデルのアーキテクチャと学習方法に移ります。このウィスパーではですね、先ほど用意した豊富なデータセットとマルチ言語、マルチタスクの学習っていうのにフォーカスするために、このモデルのアーキテクチャに関しては、音声からテキストの書き起こしっていう、Sequence to Sequenceの推論が実行できて、データ量に対してスケールするアーキテクチャとして、シンプルなエンコーダーデコーダーベースのトランスフォーマーを採用しているっていう風になっております。ここが割と、そこまで複雑ではないですね。エンコーダーの方ではですね、もともと音声、波形ですので、これをLogMelスペクトログラム、時間×周波数の表現ですね、に変換した上で、1次元のコンボリューション1Dとゲルーっていう活性化関数を通した上で、このトランスフォーマーでも一般に使われるポジショナルエンコーディングを通して、トランスフォーマーエンコーダーに入力します。この辺、Sequence to Sequenceっていうところで、大まかにはテキストの翻訳のようなゲースとざっくりは似ているかなと思いますね。その上でデコーダーですね。こちらはテキストの書き起こしを順次予測するんですけれども、順次予測しますと。なので、音声に入力してそれに対応する、this、is、aみたいなテキストを予測していきますと。シンプルにはその上なんですけど、プラス実際にはですね、この話されているオーディオの言語が何なのか、英語だったのか、日本語だったのかだったり、発話領域のタイムスタンプの予測などもテキストとまとめてトークン系列として扱って、全部丸とまとめて予測対象とするっていうことを行っているようです。なるほど。これなんか実際にツイッターとかで試した人たちがあげてたんですけど、そのこれは日本語だよって教えずにやってたのに、普通に日本語として出力ちゃんとしてくれてるみたいなやつは、そもそもモデルが学習している中に、ここの今学習で見せられているものが日本語であるっていうことも含めて学習してるってことなんですね。そうですね、はいその通りですね。すごいな。これというかあれか、学習の時にインプットは音のデータですけど、実際にモデルが学習しているのはメルスペクトログラムだから、よく音声系の人たちが画像を出している、波打ったモニョモニョモニョみたいな画像を見せて、この画像のこういう絵だったらこの発音してるよっていうテキストを予想するっていう、画像からテキストを予測するってことをやってるんですね。ざっくり言えばはい、そうなりますね。はい、そしてこの辺の全部まとめてその言語の特定とかタイムスタンプとかも全部トークンとして系列して、トークン系列として成形して、トランスフォーマーのSequence to Sequenceで学習しちゃうっていうのがうまいところであり、トランスフォーマーの結構柔軟で優秀なところなのかなっていうのは読んでて思ったところですね。そうですよね。全部ひっくるめてもう学習してってやってるってことですもんね。なので、モデルのアーキテクチャと学習方法もざっくりほとんどこれで以上って形ですね。でも元々のデータを68万時間あって多様なので学習時のデータ拡張とかも論文ではしていないっていうふうに書いてありました。あとはもうほとんど実験結果とディスカッションですね。基本的にはかなり精度がいいっていうのが大まかな内容なんですけど、やり方としてはですね、学習で68万時間学習していますと。元々の目標としてファインチューニングなどによらない反過性能を見るっていうのが目標だったので、既存の各評価用データセットの評価セットを直接推論してそれで評価を行うっていう扱いをしています。評価指標はですね、一般的な音声認識の指標であるワードエラーレート、予測した単語系列が正解とちゃんとマッチしているかっていうような指標を使っています。全部の実験結果はちょっと割愛するんですけど、わかりやすいところだと、まず英語の音声認識ですね。英語の音声認識だとLibre speechっていうデータセットが一般的なようですと。このLibre speechだけを見るとですね、必ずしもWISPRっていうのが既存のLibre speechで教師ありで学習したモデルの精度には及んでいませんと。ですけども一方でそれを踏まえて他のデータセットも含めた平均的な反過性能を見てみましょうと。データセットはいくつかあるわけですけど、WISPRで全部評価してみるっていうのと、既存の教師ありモデルでLibre speechで学習したモデルを他のいろんなデータセットでも評価してみましょうと。それによって各データセットまたがった平均的な反過性能を見てみましょうっていう扱いだと非常に優れた認識性能でなっていて、人の認識性能、この論文の中で書かれていたのは著者の一人のアレックラドフォードさんっていうかなり有名な方の認識性能も匹敵しているっていう風に書かれています。これは著者の人が同じデータセットを聞いて、ちゃんと認識できたかっていう精度もとってるってことなんですよね。多分そうですね。面白い。なるほど。既存のモデル適応しありとかファインチューニングベースのアプローチだと、チューニングしたデータセットだとスイーパーヒューマンっていうようなレベルって言われていても他のデータセットも含めた反過性能っていう点だとイマイチになるよねっていう風に言われています。じゃあその英語の次に多言語の音声認識ですね。それでもやっぱり優れた認識性能が確認されましたと。で面白いのがですね、この学習データ68万時間あって、その中に各言語のデータ量ばらつきがあって、やっぱり英語が一番多くて、マイナナ言語だと結構少ないものがあるんですけど、その双観、学習データの量と、各言語の学習データの量とその言語ごとのエラーの双観っていうのを見てみると、すごいかなりきれいに双観が取れますと。両体数グラフでかなりまっすぐな双観があって、やっぱりデータを増やすほどちゃんとエラーレートの精度もスケールしていくよねっていうのがわかったっていう風に報告されています。でもデータ量が正義みたいな世界なんですね。そうですね。こんななんかきれいに双観出るんだって思いましたね。確かに。ただ強いて言えば、インドとかヨーロピアン系から離れた言語だと、ちょっとデータを増やしても若干精度が上がりづらくて、この辺は改良の余地ありだよねみたいな風には言われていますね。これって実際にその68万時間のうちの英語は何時間分でとかって内訳も出てるんですか?多分あったと思いますね。半分以上が確か英語で、あ、そんなに?プロットを見ると日本語が1万時間くらいで、少ないのだと10時間くらいしか、10時間1時間しかないようなのもあるっていう風になっていますね。日本語は1万時間なんだ。なんかすごい精度いいって言うけど、68万時間のうちの1万時間分しかデータはなかったんですね。そうですね。そんなもんなんだ。そんなもんって言っても全然多いんだろうけど、1万時間分の日本語の音声の、しかも教師ありってなると。これもいろいろ実験してるんですけど、さらっと説明していくと、他に入力音声にノイズが乗ってしまっても既存モデルよりロバストですっていう結果もあります。であと既存の研究成果以外に、商用の音声書き起こしサービスとかと比べても遜色ない性能ですよねっていうのも報告されています。それとは別に人間のプロの書き起こしとも比べてみたときにそれにもかなり匹敵してますねっていうような結果も報告されていました。ここまで実験結果ですね。ノイズにも強いっていうのは確かにデベロッパーズIOでブログが書かれていて、それなんかわざわざオンライン会議の音声のものに扇風機の音を乗せて合成して、それでベースモデルとミディアムとラージのモデルでノイズにどれぐらい堅牢なのかみたいな実験とかされてましたね。確か結果はベースだと扇風機の音入れたらちょっと下がっちゃうけど、ミディアムとかラージはほとんど影響なかったっていうぐらいノイズ除去してるみたいな感じで面白い結果でしたね。エンジニアだとそういう公開されているものに意地悪したくなるのはちょっとわかるなって思いますね。はい。なので結構いろんな実験結果でかなり精度高く、かつかなりロバストっていうのが報告されていました。一方でリミテーションとかフューチャーワークっていうのもいくつか語られています。今回先ほどデータセットの準備っていうところで少し触れたんですけど、学習データは基本30秒ごとで学習しています。ウィスパーは長い音声を処理するときにはどうしているかっていうと、わりだと内部に30秒ごとにスライドしながら処理するっていうことをやっています。そのときにBeam SearchとかHeuristics、工夫をいくつか用いているんですけど、ここはまだまだ自然につないで長い音声を書き起こしできるように改善の余地がありますよねって言われています。もう一個ですね、まだ完璧じゃないんだなって思った点として、特に長い音声だと人間ならやらないような間違い。書き起こしのテキストがループしたり、最初とか最後の内容を無視していたり、明らかに存在しないような単語がテキストに現れるっていうことも報告されていて、この辺はより優れたデコーディングが必要ですよねって報告されていました。あとは細かいところだと、モデルアーキテクチャーは今回かなりシンプルなエンコーダー・デコーダー・トランスフォーマーですっていうところだったり、データ拡張もしてないですっていうところだったりあるので、その辺の改善だったり、あとは既存の自己教師あり学習とかと組み合わせるっていうのも応用先としてはありですよねっていう風に書かれていました。なるほど。これってそういえばOSSだからコード取ってきて回せますけど、CPUとかでもベースモデルだったら回るみたいな、結構軽いんですよね。確かなんかそんな話だった気がする。そうですね。なんかそれぞれメモリの大きさ書いてあって、一番大きいのでも10GBとかだった気がするので、回る分にははい。もちろん処理はちょっと遅くなっちゃうかなと思いますけど、回る分には回るのかなと思いますね。GPU持ってきたらもっと早く回るってことですよね。そうですね。はい。もちろんそのはずだと思います。なるほどな。なんかTwitterであと見たのは、M1Macで、リビルドの宮川さん、M1Macで3時間ぐらいの音声を文字起こししようとしたら、結構なんか丸一日かかったみたいなツイートをされた気がしますね。でもちょっと大きいモデルだったのかな。ラージじゃなくて、ミディアムかベースみたいな話だったと思いますけど、長い音源になったら、そのCPUとかだったら全然時間かかるんだなっていうっぽいですね。なるほどなるほど。そうですね。はい、ありがとうございます。以上を含めてまとめに移りますと。最初にお話しした通り、Whisper、豊富な多言語のデータセットでシンプルなエンコーダー・デコーダーベースのトランスフォーマーモデルでマルチタスクの学習を行いましたと。結果的に人間に匹敵する精度とロバスト性の音声認識能力を獲得できましたという風に報告されていますと。途中途中もちょっと言ったんですけど、私の感想としては、豊富なデータセットを用意しつつ、それだけじゃなくて、できるだけ品質のデータは除外していたりするところだったり、英語の書き起こし以外にも多言語の書き起こしだったり、言語の特定などいろんなタスクっていうのをシンプルなシークエンス・トゥー・シークエンスの枠組みに落とし込んで、スケーラブルなモデルを獲得しているという点がかなり貢献として大きいのかなという風に感想として思いましたというところですね。これって特に何がすごかったかっていう意味で言うと、ゴミっぽいデータはちゃんと排除した上で、かなり膨大な教師ありの音声データを使ったからっていう貢献が結構差分としてでっかいんですかね、従来研究とは。そうですね、やっぱりそれ大きいと思いますね。あとはそれでマルチタスクの学習をうまくシンプルな形に落とし込んで、シンプルな変更のデコーダー・トランスフォーマーで学習しているっていうのが結構大きいんじゃないかなと思います。なんかモデルはそんなに大したことない風に聞こえたんですけど、いやモデルもちゃんとマルチタスクのっていうところですごいやつなんだよっていうことなんですかね。そうですね、たぶんこのモデルのアーキテクチャだけで言えば割と結構簡単に実装できるんじゃないかなと思いますね。そのマルチタスクのデータをうまく加工を整形するところが割と難しそうかなって風に思いましたね。あとはじめの話なんですけど、気になったのはGoogleの100万時間の方で学習したBigSSLでしたっけ。あれは精度もなんか言うほど良くなかったって話だったんですけど、そっちのBigSSLの方はそしたらデータ量はちゃんとあったのにモデルがなんかあんまりいけてなかったってことになるんですかね。もっといろいろ違いはあると思うんですけど。 そうですね、たぶんやっぱり教師なしだとちょっと限界があるのかなっていうのと、結局そのファインチューニング先にオーバーフィットしちゃうっていうのが、このWispertの差分としてはかなり大きいんじゃないかなって風に思いますね。たぶん精度もそんなに悪いってことではないと思うんですけど、汎用ないろんなシーンのデータにうまく繁華して推論できるって意味だと、教師ありで広大なデータを用意して学習したWispertに軍配が上がっているのかなと思いますね。そんな感じなんですね。 まあこれまたベースとミディアムとラージって言って結構使いやすい形でローカルマシンでも回せる形で提供してくれてるから、またそこもなんかこんだけ話題に差が出たっていうのは貢献してるんですかね。ステーブルディフュージョンと一緒で姿勢の人たちも回しやすいっていう意味で。 結局そのBigSSL公開されたとしても、教師なしの状態で公開されたとしても、自分で使うにはファインチューニングしなきゃいけないですし、ファインチューニングされたものだと、やっぱりそのファインチューニング先のデータにオーバーフィットするっていう形になっているので、完全にすぐクローンして使えるWispertがかなり注目されているのかなと思いますね。なるほど。いやありがとうございます。ちょっとこれ、この収録までに白金FM試しにやってみ、文字起こししてみて、すごいっていう話をここに添えられたらいいと思ったが間に合わずなんで、ちょっと追加の宿題でやっとこうと思います。そうなんですよね。ちなみに今まで白金FMは投稿するときに文字起こししたワードクラウドをツイートにくっつけてたんですけど、あれはずっと、ずっとというか一応公開されている文字起こしのAzureとAWSとGCPの文字起こしのAPIを使って比較して、一番AWSのやつが使いやすかったので、文字起こしの品質も、結構どんぐりの性比べ的な感じではあるんですけど、マシだったからそれ使ってたんですけど、今回からはWispertで全部回したいなと思いますね。そんなに違うんだったらということで、めっちゃ楽しみ。そうですね。ちょっとそれはぜひ見てみたいですね。そうですよね。あと、今までのクラウドサービスって、やっぱりクラウドにデータを置くので、社内の秘密の会話とかは簡単に上げて文字起こししようみたいなのできなかったですけど、今回これもローカルで回せるから、社内用のおもちゃツール作って、そこに投稿したらみんな議事録文字起こしできるよみたいな、ラージュモデルで結構重いけどみたいなも含めてできるから、これ、何て言うんですか、これみんなが求めてたやつで、すごい業務効率みたいなのにも、もっと使えると思って、もっと話題になると思ったんですけど、なんか微妙にちょっと下火になってきました。また人盛り上がりした後に。そうですね。これの利活用事例みたいなの、めっちゃ楽しみにしてますね。タイムラインにいっぱい出てくるの。かなりそうですね、応用の余地は広そうな気はしますね。あと、これちょっと関係ないんですけど、結構似たようなタイミングで、Facebookの研究開発部署が、R&Dのところが作った、なんて読むんだろう、Demucas V3かな。ちょっと読み方間違えてる気はしますけど、それ、音源分離ができるっていうやつが出てて、かなり綺麗に、バンドの音とボーカルの音切り分けたりとかできてたんですけど、なんかそんなことも、かなり綺麗にできて、文字起こしもちゃんとできるんだったら、音声系のデータ、基本的にもう制覇したと言っても過言ではないのではって思ったりするんですけど、界隈の人たち的にはどうなんですかね。いやー、どうなんですかね。ちょっと僕も正直、音声はほとんど触れてなかったので、わからないところではあるんですけど、あれもとはいえ、結構、そうですね、例えば、ウィスパーだと、明らかに話されてない単語が出てくるとか、ループしてしまうとか、それ気重いですよね。かなり根深い問題そうなので、この辺まだ改善の位置とかまだまだあるのかなと思いますね。そうですね。デコーダー側が改善されて、もうちょっと長く学習と推論を行うような感じになってきたら、またあっという間に良いやつが出るんでしょうね。そうですね。かなり期待できると思います。ありがとうございました。ちょっとぜひ早めに試して、こんなにもAWSとかのAPIと違うっていうのを比較したいですね、自分も。そうですね。楽しみですね。じゃあ、結構サクッと終わっちゃいましたが、こんなとこですかね。ではでは、今回はこれで終わりたいと思います。ありがとうございました。'
